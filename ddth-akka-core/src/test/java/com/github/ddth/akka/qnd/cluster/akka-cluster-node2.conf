# Sample configuration for Akka system (cluster mode).
#
# You can include other configuration files in this main application.conf file:
#include "extra-config.conf"
#
# You can use environment variable for substituting:
#mykey = ${JAVA_HOME}

akka_system.name = "my-actor-system"
akka {
    # To enable akka logging: remember to add <logger name="akka.actor" level="INFO" /> to logback config file
    log-config-on-start = false
    jvm-exit-on-fatal-error = true

    log-dead-letters = 0
    log-dead-letters-during-shutdown = off
    
    remote {
        log-remote-lifecycle-events = off
        netty.tcp {
            hostname = "127.0.0.1"
            port     = 9052
        }
    }

    cluster {
        # User "roles" to group nodes within a cluster.
        roles      = ["master"]
        name       = ${akka_system.name}
        seed-nodes = [
            "akka.tcp://"${akka_system.name}"@127.0.0.1:9051"
        ]

        scheduler {
            tick-duration   = 5ms
            ticks-per-wheel = 1024
        }

        distributed-data {
            gossip-interval = 10ms
        }

        pub-sub {
            # Actor name of the mediator actor, /system/distributedPubSubMediator
            name = distributedPubSubMediator

            # Start the mediator on members tagged with this role.
            # All members are used if undefined or empty.
            role = ""

            # The routing logic to use for 'Send'
            # Possible values: random, round-robin, broadcast
            # "random" seems to be better than round-robin
            routing-logic = "random"

            # How often the DistributedPubSubMediator should send out gossip information
            gossip-interval = 10ms

            # Removed entries are pruned after this duration
            removed-time-to-live = 120s

            # Maximum number of elements to transfer in one message when synchronizing the registries.
            # Next chunk will be transferred in next round of gossip.
            max-delta-elements = 3000

            # The id of the dispatcher to use for DistributedPubSubMediator actors.
            # If not specified default dispatcher is used.
            # If specified you need to define the settings of the actual dispatcher.
            use-dispatcher = "akka.actor.default-dispatcher"
        }

        # auto downing is NOT safe for production deployments.
        # you may want to use it during development, read more about it in the docs.
        #auto-down-unreachable-after = 120s

        # Disable legacy metrics in akka-cluster
        metrics.enabled=off

        # Sigar native library extract location during tests.
        # Note: use per-jvm-instance folder when running multiple jvm on one host.
        # metrics.native-library-extract-folder=${user.dir}/target/native
    }

    # Enable metrics extension in akka-cluster-metrics.
    # It is recommended to load the DistributedPubSub extension when the actor system is started.
    # Otherwise it will be activated when first used and then it takes a while for it to be populated.
    extensions=["akka.cluster.metrics.ClusterMetricsExtension", "akka.cluster.pubsub.DistributedPubSub"]    

    actor {
        provider = "akka.cluster.ClusterActorRefProvider"
        
        serializers {
            ddth = "com.github.ddth.akka.cluster.serialization.DdthAkkaSerializer"
        }
        
        serialization-bindings {
            "com.github.ddth.akka.scheduling.TickMessage" = ddth
            "com.github.ddth.akka.cluster.DistributedDataUtils$DDLock" = ddth
        }
        
        # Thread pool for worker tasks
        # Lookup with actorSystem.dispatchers().lookup("akka.actor.worker-dispatcher")
        worker-dispatcher {
            type       = "Dispatcher"
            executor   = "fork-join-executor"
            # Throughput defines the maximum number of messages to be
            # processed per actor before the thread jumps to the next actor.
            # Value of 1 means "as fair as possible"
            throughput = 1
            # Configuration for the fork join pool
            fork-join-executor {
                # Min number of threads to cap factor-based parallelism number to
                parallelism-min    = 2
                # Parallelism (threads) ... ceil(available processors * factor)
                parallelism-factor = 2.0
                # Max number of threads to cap factor-based parallelism number to
                parallelism-max    = 64
            }
        }        

        # Override some configurations of the default dispatcher
        default-dispatcher {
            throughput = 1
            executor = "fork-join-executor"
            fork-join-executor = {
                # Settings this to 1 instead of 3 seems to improve performance.
                parallelism-factor = 1.0
                parallelism-max    = 64
                task-peeking-mode  = FIFO
            }
        }
    }
}
